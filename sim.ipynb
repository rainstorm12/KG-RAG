{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法1：余弦相似度直接计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-30 21:21:08.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtext2vec.sentence_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1mUse device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel, semantic_search\n",
    "import sentence_transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "#文件读取\n",
    "document_name = 'somuut'\n",
    "document_path='./data/'+document_name+'.txt'\n",
    "vector_path = './data/'+document_name+'.pt'\n",
    "with open(document_path, 'r', encoding='utf-8') as file:\n",
    "    document = file.readlines()\n",
    "document = [line.strip() for line in document]\n",
    "sequence_id = np.arange(0, len(document)) \n",
    "embedder  = SentenceModel('shibing624/text2vec-base-chinese')\n",
    "if os.path.exists(vector_path):\n",
    "    embeddings = torch.load(vector_path)\n",
    "    print(\"already load vector\")\n",
    "else:\n",
    "    embeddings = embedder.encode(document, convert_to_tensor=True)\n",
    "    torch.save(embeddings, vector_path)\n",
    "    \n",
    "def ranking_api(query,document,embedder,embeddings,top_k):\n",
    "    if not query: \n",
    "        zero_score = [0.0 for i in range(0,len(document))]\n",
    "        return document,zero_score\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    sim_scores = sentence_transformers.util.cos_sim(query_embedding, embeddings)[0]\n",
    "    #对得分进行排序\n",
    "    # top_k = len(document)\n",
    "    top_k_cat = torch.topk(sim_scores, k=top_k)\n",
    "    top_k_score,top_k_idx = top_k_cat[0],top_k_cat[1]\n",
    "    top_k_name = [document[top_k_idx[i]] for i in range(0,top_k)]\n",
    "    return top_k_name,top_k_score,top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['天然气管道应按巡检计划定期巡检，运行状况应符合下列规定：',\n",
       "  '天然气管道应按巡检计划定期巡检',\n",
       "  '管道巡检应包括下列内容：',\n",
       "  '天然气管道巡检计划应根据管线的压力等级、供应用户情况',\n",
       "  '天然气管道应定期检查放散系统，',\n",
       "  '天然气管道的运行维护及安全管理尚应符合国家现行标准《城镇燃气设施运行、维护和检修安全技术规程》CJJ 51、《城镇燃气管网泄漏检测技术规程》CJJ/T 215和《燃气系统运行安全评价标准》GB/T 50811的有关规定。',\n",
       "  '综合管廊内天然气管线的运行维护及安全管理还应符合国家现行标准《城镇燃气设施运行、维护和检修安全技术规程》CJJ 51、《城镇燃气管网泄漏检测技术规程》CJJ 215和《燃气系统运行安全评价标准》GB/T 50811的有关规定。',\n",
       "  '排水管道的巡检应包括下列内容：',\n",
       "  '给水排水系统巡检内容及要求应符合表5．7．4的规定。',\n",
       "  '应定期检查天然气管线放散管的接地可靠性、牢固程度和管道通畅性。'],\n",
       " tensor([0.7777, 0.7516, 0.7287, 0.7189, 0.7080, 0.7064, 0.6882, 0.6797, 0.6785,\n",
       "         0.6779], device='cuda:0'),\n",
       " tensor([207,   3, 630, 728, 290, 364, 187, 314,  24, 108], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"天然气管道巡检有哪些标准\"\n",
    "ranking_api(query,document,embedder,embeddings,top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法2：faiss检索库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\faiss\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "already load vector\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def vector_search(query, model, index, num_results=10):\n",
    "    \"\"\"Tranforms query to vector using a pretrained, sentence-level\n",
    "    DistilBERT model and finds similar vectors using FAISS.\n",
    " \n",
    "    Args:\n",
    "        query (str): User query that should be more than a sentence long.\n",
    "        model (sentence_transformers.SentenceTransformer.SentenceTransformer)\n",
    "        index (`numpy.ndarray`): FAISS index that needs to be deserialized.\n",
    "        num_results (int): Number of results to return.\n",
    " \n",
    "    Returns:\n",
    "        D (:obj:`numpy.array` of `float`): Distance between results and query.\n",
    "        I (:obj:`numpy.array` of `int`): Paper ID of the results.\n",
    " \n",
    "    \"\"\"\n",
    "    vector = model.encode(list(query))\n",
    "    D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def id2details(df, I, column):\n",
    "    \"\"\"Returns the paper titles based on the paper index.\"\"\"\n",
    "    return [list(df[df.id == idx][column]) for idx in I[0]]\n",
    "\n",
    "#文件读取\n",
    "document_name = 'somuut'\n",
    "document_path='./data/'+document_name+'.txt'\n",
    "vector_path = './data/'+document_name+'.npy'\n",
    "\n",
    "with open(document_path, 'r', encoding='utf-8') as file:\n",
    "    document = file.readlines()\n",
    "document = [line.strip() for line in document]\n",
    "sequence_id = np.arange(0, len(document)) \n",
    "\n",
    "# Instantiate the sentence-level DistilBERT\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# Check if CUDA is available ans switch to GPU\n",
    "if torch.cuda.is_available():\n",
    "   model = model.to(torch.device(\"cuda:0\"))\n",
    "print(model.device)\n",
    "\n",
    "if os.path.exists(vector_path):\n",
    "    embeddings = np.load(vector_path)\n",
    "    print(\"already load vector\")\n",
    "else:\n",
    "    # Convert abstracts to vectors\n",
    "    embeddings = model.encode(document, show_progress_bar=True)\n",
    "    embeddings = np.array([embedding for embedding in embeddings]).astype(\"float32\")\n",
    "    np.save(vector_path, embeddings)\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])# Step 1: Instantiate the index\n",
    "index = faiss.IndexIDMap(index)# Step 2: Pass the index to IndexIDMap\n",
    "index.add_with_ids(embeddings, sequence_id)# Step 3: Add vectors and their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"天然气管道巡检有哪些标准\"\n",
    "# Querying the index\n",
    "D, I = vector_search([user_query], model, index, num_results=10)\n",
    "print([document[i] for i in I.tolist()[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhixi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
