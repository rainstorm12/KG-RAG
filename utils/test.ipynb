{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b-int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_entity_extraction(sentence):\n",
    "    entity_type = \",\".join([\"巡查项目\",\"巡查内容\",\"巡查方法\",\"巡查周期\"])\n",
    "    entity_format=\"[实体1,实体2,实体3,...]\"\n",
    "    return f\"\\\n",
    "说明：从给定的输入文本中提取实体，这些实体可能的类型包括“{entity_type}”,提取的实体最后以{entity_format}的格式回答。\\n\\\n",
    "示例如三个反引号内所示：\\n\\\n",
    "```\\n\\\n",
    "输入文本：“管道泄水及冲洗水的封堵应该怎么做？”\\n\\\n",
    "输出结果：[管道,泄水及冲洗水,封堵]\\n\\\n",
    "```\\n\\\n",
    "注意：输出结果是从用户的输入文本中提取实体,输出结果严格以{entity_format}的格式回答,不需要生成其他任何多余的内容。\\n\\\n",
    "此时用户的输入文本是：“{sentence}”\\n\\\n",
    "输出结果：\"\n",
    "sentence = \"管廊中渗漏水的维护该怎么做？\"\n",
    "prompt = prompt_entity_extraction(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "说明：从给定的输入文本中提取实体，这些实体可能的类型包括“巡查项目,巡查内容,巡查方法,巡查周期”,提取的实体最后以[实体1,实体2,实体3,...]的格式回答。\n",
      "示例如三个反引号内所示：\n",
      "```\n",
      "输入文本：“管道泄水及冲洗水的封堵应该怎么做？”\n",
      "输出结果：[管道,泄水及冲洗水,封堵]\n",
      "```\n",
      "注意：输出结果是从用户的输入文本中提取实体,输出结果严格以[实体1,实体2,实体3,...]的格式回答,不需要生成其他任何多余的内容。\n",
      "此时用户的输入文本是：“管廊中渗漏水的维护该怎么做？”\n",
      "输出结果：\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(output.logits, dim=-1)[:,-1]\n",
    "next_token_id = torch.argmax(probabilities, dim=-1) \n",
    "next_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" 概率为: 0.669921875\n",
      "[' 概率为: 0.134033203125\n",
      "[ 概率为: 0.134033203125\n",
      "[' 概率为: 0.021209716796875\n",
      "[ 概率为: 0.01551055908203125\n",
      "[\" 概率为: 0.00531768798828125\n",
      "无法 概率为: 0.005237579345703125\n",
      "[- 概率为: 0.00417327880859375\n",
      "〔 概率为: 0.003078460693359375\n",
      "[\\ 概率为: 0.0022869110107421875\n",
      "[[ 概率为: 0.0018968582153320312\n",
      "[] 概率为: 0.0003535747528076172\n",
      "在 概率为: 0.0003399848937988281\n",
      "无 概率为: 0.0003294944763183594\n",
      "只能 概率为: 0.00015091896057128906\n",
      "不需要 概率为: 0.00014972686767578125\n",
      "管道 概率为: 0.00012803077697753906\n",
      "<0x0A> 概率为: 0.00010454654693603516\n",
      "([ 概率为: 9.66787338256836e-05\n",
      "［ 概率为: 9.22083854675293e-05\n",
      "管 概率为: 8.20159912109375e-05\n",
      "[$ 概率为: 6.389617919921875e-05\n",
      "的唯一 概率为: 5.21540641784668e-05\n",
      "[] 概率为: 3.904104232788086e-05\n",
      "没有 概率为: 3.904104232788086e-05\n",
      "【 概率为: 3.612041473388672e-05\n",
      "得不到 概率为: 3.3915042877197266e-05\n",
      "需要在 概率为: 3.0159950256347656e-05\n",
      "无需 概率为: 2.9027462005615234e-05\n",
      "=[ 概率为: 2.5391578674316406e-05\n",
      "可以 概率为: 2.5391578674316406e-05\n",
      "{\" 概率为: 2.5212764739990234e-05\n",
      "唯一的 概率为: 2.1398067474365234e-05\n",
      "将 概率为: 2.1398067474365234e-05\n",
      "[[ 概率为: 2.008676528930664e-05\n",
      "[]; 概率为: 1.817941665649414e-05\n",
      "分别在 概率为: 1.7464160919189453e-05\n",
      "糜 概率为: 1.5556812286376953e-05\n",
      "只 概率为: 1.4483928680419922e-05\n",
      "【 概率为: 1.3172626495361328e-05\n",
      "分别是 概率为: 1.1444091796875e-05\n",
      "在其他 概率为: 1.1026859283447266e-05\n",
      "我会 概率为: 1.0371208190917969e-05\n",
      "只有一个 概率为: 1.0013580322265625e-05\n",
      "[: 概率为: 9.775161743164062e-06\n",
      "main 概率为: 9.655952453613281e-06\n",
      "不含 概率为: 8.761882781982422e-06\n",
      "分别为 概率为: 8.702278137207031e-06\n",
      "{ 概率为: 7.867813110351562e-06\n",
      "[:, 概率为: 7.808208465576172e-06\n"
     ]
    }
   ],
   "source": [
    "top_values, top_indices = torch.topk(probabilities.view(-1), 50)\n",
    "for i in range(0,len(top_indices)):\n",
    "    print(tokenizer.decode(top_indices[i]),\"概率为:\",top_values[i].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
